Implementing modules.

Syntax:

   import mod
   import mod as alias
   from mod import item_list
   from mod import *

Issues:

The first time a module is imported into a running application it is "executed",
which may have side effects (eg top-level statements). Subsequent loads do not
execute those effects again. Thus the loaded module is cached. A corollary is
that modules are identified uniquely by their name.

Python supports "packages" which make the story slightly more complex. To
simplify things it might make sense to ignore packages in the first pass.

An import of a module binds one or more variables in the local scope of the
importing statement. This is
problematic for the "from mod import *" syntax, because it does not name the
variables that it binds - they are simply the variables exported by the
imported module. So far our scheme has been to compile
Python variable names into Haskell variable names (where the Haskell variable
is bound to an IORef). For performance reasons we are trying to avoid using
strings (and an explicit environment) to handle variable names - although such
an approach would make it relatively easy to handle this tricky kind of import.

Fortunately this tricky kind of import is only allowed at the top level of
a module. Given this limitation, a compromise might be possible. The idea
is to maintain a global environment, which maps (string) names to object
references (IORef Object, aka ObjectRef):

   type GlobalEnv = Map String ObjectRef

All variables referred to in a module must be declared first in the
compiled Haskell code. Top level
declarations can check the global environment. If a variable of the same name
was imported then the corresponding ObjectRef should be retrieved from the
table. Otherwise a new ObjectRef should be allocated and the table should
be updated. Reads from variables remain unchanged.

For instance, the Python code (at the top level)

   x = 12
   print(x)

is compiled to:

   = do _s_x <- var "x"
        _s_x =: 12
        _t_0 <- read _s_print
        _t_1 <- read _s_x
        _t_0 @@ [_t_1]

where the 'var' primitive does the variable declaration, and has type:

   var :: Ident -> Eval ObjectRef

which would be something like this in pseudo-code:

   var s = do
      maybeGlobal <- lookupGlobalEnv s
      case maybeGlobal of
         Nothing -> do
            ref <- newIORef (error $ "undefined variable: " ++ s)
            updateGlobalEnv s ref
            return ref
         Just ref -> return ref

An imported module should update the global environment with the variables
that are imported from it. In the case of the "import *", all variables are
imported, whereas the other kinds of imports are more restrictive. For instance,
"import Foo" just introduces the "Foo" variable into scope (which will be
bound to a Module object).

Side note: if we ever wanted to we could presumably extend this scheme to support
'from Foo import *' at other nesting levels, by including a suitably nested
variable enviroment.

One question is what to do with threads? Does each thread have its own
"thread global" environment? Seems plausible.

Other issues to consider is that module imports have dynamic behaviour.
They can be evaluated in nested scopes and under conditional statements:

   if test:
      import Foo
   else:
      import Bar

So it is undesirable to require them to be statically known and linked.
That means we can't use Haskell's import facility to implement Python's
import facility (in its full glory). (Though a static import facility
could be supported and may be a useful extension). A promising workaround is to use
dynamic loading via something like the plugins library. The idea would
be that each compiled module exports a single entity called, say,
init, which would have a type like:

   init :: Eval Object

The resulting object would be a Module that contains a
dictionary mapping all its members to objects.

So a Python statement like:

   import Foo

would be compiled to:

   obj <- importModule "Foo"
   _s_Foo <- var "Foo"
   _s_Foo =: obj

or maybe (this is probably better):

   _s_Foo <- importModuleRef "Foo"

where

   importModule :: String -> Eval Object

and/or

   importModuleRef :: String -> Eval ObjectRef

with pseudo code:

   -- this just handles the simple case of: import Foo
   importModule name = do
      maybeImported <- lookupModule name
      case maybeImported of
         Just obj -> return obj
         Nothing -> do
            tryCompiled <- compileModule name
            case tryCompiled of
               Left e -> raise ("compilation error: " ++ show e)
               Right obj -> return obj

   compileModule :: String -> Eval Object
   compileModule name = do
      maybePath <- findModulePath name
      case maybePath of
         Nothing -> raise ("could not find module")
         Just path -> do
            compiled <- isCompiled path
            if compiled
               then liftIO $ load path "init"
               else do
                  compileResult <- compileToObj path
                  case compileResult of
                     Nothing -> liftIO $ load path "init"
                     Just err -> raise err

assuming:

   load :: FilePath -> String -> IO a

or something like that.

Each Python module should be compiled to a Haskell binding defining
and init function which is the only variable exported from the
Haskell module:

   init :: Eval Object
   init = do
      ... compiled stuff ...
      -- should be hashed strings below
      mkModule [("x", _s_x), ..., ("z", s_z)]

where mkModuleObject builds the object for the module from
the top variables defined in it:

   mkModule :: [(Hashed String, ObjectRef)] -> Eval Object

We have to compile the Python code to object code and
then dynamically load the object code. This raises the question:

   Should we call the compiler (from the running program) as a shell call,
   or should we compile the compiler into the runtime library?

A shell call keeps the runtime and the compiler separate but at what advantage?
Will make the resulting executable smaller. But we could in theory
dynamically link the compiler to the excecutable. The space saving is not so
compelling because we still need to have the compiler around anyway. Nonetheless
static linking the compiler to the runtime would be undesirable. Does GHC support
dynamic linking everywhere?

It might be (slightly?) faster for the compiler to be called directly rather
than from a shell call. It might also be more portable.

Plan: see if we can compile the compiler into the runtime. See if it works and
see if the size of executables is okay. Hope for dynamic linking to work.

This will require us to build a berp (compiler) library from the cabal file.
Both the command line front end and the runtime will link to the library.

What about the main function?

Simple solution: the berp executable just dynamically loads the module that
was mentioned on the command line, something like:

   main :: IO ()
   main = do
      args <- getArgs
      let pySrc = getPySrc args
      init <- importModule pySrc
      runStmt init

How should the interpreter work? Maybe we can also use dynamic linking. The idea
is to compile each new statement into a temporary module and then dynamically
load it into the running program. The main issue to solve is how to bind the
free variables in the statement to their values from the running program? One
possible solution is to compile each statement into a function (closure) that
binds all the free variables. Something akin to the way we propose to handle
'from Foo import *'. It seems appealing to try to implement the interpreter this
way. Perhaps there is a performance issue due to loading times? Maybe we can
avoid touching the file system?

We could use the dynamic import facility to link the compiled program to the base library. That is
we compile a special module in the base library and dynamically load it a runtime. Or maybe it is just
better to statically import it? Again the availability of dynamic linking makes a difference to the
size of the resulting executable.

It looks like dynamic loading might invalidate any global state in the program.
Though it might be a bug, currently the stdandard IO devices do not seem to
persist as expected across a dynamic load (this is evident when the stdout is
redirected on the shell, after a dynamic load it seems the redirection is
lost and the output disappears). There are a few cases of global state in the
current implementation which could do with a revision in light of this
discovery. We use unsafePerformIO to make some otherwise effectful operations into
globals. This is safe because we are careful to ensure that the effects are
benign, such as allocating IORefs, but even so, the use of unsafe operations
seems less than ideal. A few observations:
   - If these things are truly constant, then we should never need the IO monad. Hopefully global
     constant bindings should suffice, immutable data structures.
   - For most/all such global constants, the point of making them global is to simplify
     scoping issues (they are in scope everywhere). An alternative approach is to bring them
     into scope by importing them, just as the import mechanism will bring other things into scope.
     We could pretend that every Python module has an implicit:

        from builtins import *

     at the top, where builtins is a special module which is part of the base library
     implementation. This might ultimately be a cleaner solution to the problem. Then everything
     will be in the Eval monad, and there will be no unsafePerformIO. However, a more static
     approach might be more efficient as it seems that dynamic linking is a little bit slow.
     Obviously berp will have to know how to find builtins.o in order to link it in.

Module naming strategy. Given a module called Foo.py, what is the name of the resulting Haskell module?
Haskell modules have an internal name and the filename is somewhat ancilliary. However, it tends to make
things easy for GHC if the module name is the same as the file name. This can cause trouble because of
capitalisation (and maybe there are other issues). The proposal is to prepend "Berp_" onto the front of every
name. For example, Foo.py will become (Berp_Foo.hs, module Berp_Foo), whereas foo.py will
become (Berp_foo.py, module Berp_foo). Even more name mangling might be needed if python allows characters in
its name that are not allowed in Haskell modules. I'm not aware of any other issues at the moment, but maybe
there are unusual things like unicode issues to consider.

Small problem found: we don't currently do a proper job of bound methods. This causes trouble with imports
like so:
    import Foo
    Foo.f()
We treat this like this was a method call of f on Foo, which means we add Foo as the first argument,
which is not correct. We need to distinguish between method lookups and module attribute lookups.

Question: what is considered to be defined at the top-level of a module? Obviously all top-bound variables,
but what about things imported from other modules? Seems like the answer is yes.
The semantics of which top-level variables are bound to the module object is probably a dynamic property
of the program, so our static mkModule technique is probably wrong. For example, consider this top-level code:

   if cond:
      x = 5
   else:
      y = 12

The value of cond will determine whether x or y is bound in the module. Currently we compile this code so that
both x and y are considered in scope, which is clearly wrong. The solution is to rejig the way the variables in
top scope are declared. First we need to push the declarations into the inner scopes where they are defined. Second we need to make the variable declaration operator modify some state value in the eval monad. Third we
need to collect all the dynamically declared variables at the end of the evaluation of a module and bind
them to the resulting module object. This change could break things, so we need to think about it carefully.

Implementation strategy:

   1. Add a module data constructor to the Object type. Done.
   2. Modify compilation of a Python module to use the init binding, (Done)
      and implement mkModule (Done). Move compiler code into library (Done).
      Implement a basic importModule (Done).
      Need to handle the main function to evaluate the whole program, this will
      basically just call importModule (Done).
   3. Get module name mangling to work. (Done)
   4. Get the simple case of "import Foo" to work. Don't worry about
      caching imports. Don't worry about the search path. Just find
      modules in the current directory. (Done)
   5. Avoid recompiling haskell modules that are already compiled. (Done)
   6. Add import caching (Done).
   7. Get top-level variable declarations working dynamically, and thus
      fix the way they are bound to the module object, see notes above (Pending)
   8. Implement the more difficult case of "from Foo import (x,y,z)"
   9. Implement the hardest case of "from Foo import *".
   10. Add search path for files.
   11. Get it working in the interpreter. (Maybe skip if tricky).
   --- stop here and release the code ---
   10. Consider what's needed for packages.
