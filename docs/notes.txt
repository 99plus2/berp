Random thoughts whilst implementing berp.

--------------------------------------------------------------------------------

The library StringTable.Atom looks like it might be useful for
implementing string lookups.

--------------------------------------------------------------------------------

Compiling variables.

Ideally we would compile Python variables into Haskell variables. But Python
variables can be updated inside loops and closures, whereas Haskell variables
cannot. The next best thing is to use IORefs. A promising scheme is to 
compile each Python variable into a Haskell variable bound to an IORef. A read
from a Python variable becomes a readIORef, likewise writeIORef.

A few problems still remain. Python's scoping rules are slightly different to
Haskell.

Nested references to variables can be annotated as "global" and "nonlocal", otherwise
a nested assignment creates a new local variable. The main issue is that Python
variable binding sites are potentially ambiguous (there is no separate syntax
to introduce a new variable - assignment is used, but this is problematic when
another variable of the same name is already in scope).

Here is an example:

x = 12
def f():
   x = 144
   def g():
     global x
     x = 42
   g()
f()
print(x)

This prints 42. If you remove the "global x" then the program prints 12. 
Note that the "global x" causes the innermost x to refer to the outermost x, thus
skipping the one in the middle. Haskell's nested scoping does not have an
equivalent way to skip enclosing scope.

One possible way to avoid this is to rename global variables, in psuedo
compiled haskell:

global_x = newVariable "x" 
writeIORef global_x 12
f = def (\[] -> do
   x = newVariable "x"
   writeIORef x 144 
   g = def (\[] -> do
     writeIORef global_x 42
     )
   g @@ []
   )
f @@ []
print @@ [global_x]

This looks good but has problems with "nonlocal".

--------------------------------------------------------------------------------

Implementing generators/yield

data ControlStack =
   ...
   GeneratorContinuation
   { generator_return :: Object -> Eval ()
   , generator_object :: Object 
   , control_stack_tail :: ControlStack
   }
   ...

data Object =
   ...
   Generator
   { object_continuation :: IORef (Eval ())
   , object_stack_context :: IORef (ControlStack -> ControlStack)
   , object_identity :: Identity
   , object_type :: Object
   , object_dict :: Dictionary
   }
   ...

yield :: Object -> Eval ()
yield obj = do
   callCC $ \next -> do
      (generatorObj, context) <- unwindYieldContext
      liftIO $ writeIORef (object_continuation generatorObj) next 
      liftIO $ writeIORef (object_control_context generatorObj) context 
      generator_return top obj 

-- the next method for generators
next :: [Object] -> Eval Object
next (obj:_) = 
   callCC $ \afterYield -> 
      case obj of
         Generator {} -> 
            result <- do 
               push $ GeneratorContinuation afterYield obj
               stackContext <- liftIO $ readIORef $ object_stack_context obj
               modifyStack stackContext 
               liftIO $ readIORef $ object_continution obj 

-- used in the compilation of def generators
funGenerator :: Eval Object -> Eval Object
funGenertor body = do
   contRef <- liftIO $ newIORef (body >> return ())
   stackRef <- liftIO $ newIORef id
   generator contRef stackRef 

How much stack is saved when a yield occurs?

The Python PEP 0255 is not entirely clear about that:

  "If a yield statement is encountered, the state of the function is
   frozen, and the value of expression_list is returned to .next()'s
   caller.  By "frozen" we mean that all local state is retained,
   including the current bindings of local variables, the instruction
   pointer, and the internal evaluation stack:  enough information is
   saved so that the next time .next() is invoked, the function can
   proceed exactly as if the yield statement were just another external
   call."

What is the "internal evaluation stack"?

This example demonstrates that we don't save all the stack:

>>> def f():
...    yield 1
...    yield 2
...    1 / 0
...    yield 3
... 
>>> try:
...    i = f()
...    print(next(i))
... except:
...    print("here")
... 
1
>>> next(i)
2
>>> next(i)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 4, in f
ZeroDivisionError: int division or modulo by zero
>>> next(i)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration
>>> next(i)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration

As can be seen, the try/except is not preserved when the generator is called
subsequently.

Importantly we can also see that an exception raised during the execution
of a generator causes the generator to end and subsequent next() calls
result in StopIteration. 

--------------------------------------------------------------------------------

Comprehensions / generators

We have four kinds:
   - Generator expressions.
   - List comprehensions.
   - Set comprehensions.
   - Dict comprehensions.

Basic idea is to turn them all into generators, and for list, set and dict,
wrap the output of the generator in a constructor.

That is:

   [ x + 1 for x in e ]

becomes:

   list(x + 1 for x in e)

Then we compile generators into for loops:

So:

   list(x + 1 for x in e)

becomes:

   def gen():
       for x in e:
           yield x + 1
   list(gen())

But this is a little bit costly due to the creation of a new function
and the unnecessary use of iterators. We can specialise the construction
of the set/list/dictionary.

So instead:

   [ x + 1 for x in e ]

becomes:

   v = list()
   for x in e:
      v.append(x + 1)

and v is the value of the whole expression.

Generator comprehensions need to be treated differently, because:

   (x + 1 for x in e)

should not be:

   for x in e:
      yield x + 1

because you get a yield outside a function. Instead it should be:

   def v():
      for x in e:
         yield x + 1
   v()

But it is overkill to define a function for this. Instead we want
to compile to a version which uses mkGenerator:

   mkGenerator [[ for x in e: yield x + 1 ]]

Syntax:

   exp_comp := [comp] | {comp}_set | {comp}_dict | (comp)
   comp     := exp forpart
   forpart  := for pat in exp rest
   rest     := epsilon | ifpart | forpart
   ifpart   := if exp rest

[[ [comp] ]] =

   var = list()
   [[ comp ]]_(var,List)

[[ {comp}_set ]] =

   var = set()
   [[ comp ]]_(var,Set)

[[ {comp}_dict ]] =

   var = dict()
   [[ comp ]]_(var,Dict)

[[ (comp) ]] = mkGenerator ([[ comp ]]_(?,Gen))

[[ exp forpart ]]_(var,type) =

   [[ forpart ]]_(updater(type,var,result))

[[ for pat in exp rest ]]_(result) =

   for pat in exp:
       [[ rest ]]_(result)

[[ epsilon ]]_(result) = result

[[ if exp rest ]]_(result) =

   if exp:
       [[ rest ]]_(result)

--------------------------------------------------------------------------------

Compiling modules.

Syntax:

   import mod
   import mod as alias
   from mod import item_list
   from mod import *

Issues:

The first time a module is imported into a running application it is "executed",
which may have side effects (eg top-level statements). Subsequent loads do not
execute those effects again. Thus the loaded module is cached. So modules are
identified uniquely by their name.

Python supports "packages" which make the story slightly more complex. To
simplify things it might make sense to ignore packages in the first pass.

An import of a module binds one or more variables in the local scope. This is
problematic for the "from mod import *" syntax, because it does not name the
variables that it binds. So far our scheme in Haskell has been to compile
Python variable names into Haskell variable names (where the Haskell variable
is bound to an IORef). For performance reasons we are trying to avoid using
strings (and an explicit environment) to handle variable names - although such
an approach would make it relatively easy to handle this tricky kind of import.

Fortunately this tricky kind of import is only allowed at the top level of
a module. Given this limitation, a compromise might be possible. The idea
is to maintain a global environment, which maps (string) names to object
references (IORef Object, aka ObjectRef):

   type GlobalEnv = Map String ObjectRef

All variables referred to in a module must be declared first in the
compiled Haskell code. Top level
declarations can check the global environment. If a variable of the same name
was imported then the corresponding ObjectRef should be retrieved from the
table. Otherwise a new ObjectRef should be allocated and the table should
be updated. Reads from variables remain unchanged.

For instance, the Python code (at the top level)

   x = 12
   print(x)

is compiled to:

   = do _s_x <- var "x"
        _s_x =: 12
        _t_0 <- read _s_print
        _t_1 <- read _s_x
        _t_0 @@ [_t_1]

where the var primitive does the variable declaration, and has type:

   var :: Ident -> Eval ObjectRef

which would be something like this in pseudo-code:

   var s = do
      maybeGlobal <- lookupGlobalEnv s
      case maybeGlobal of
         Nothing -> do
            ref <- newIORef (error $ "undefined variable: " ++ s)
            updateGlobalEnv s ref
            return ref
         Just ref -> return ref

An imported module should update the global environment with the variables
that are imported from it. In the case of the "import *", all variables are
imported, whereas the other kinds of imports are more restrictive. For instance,
"import Foo" just introduces the "Foo" variable into scope (which will be
bound to a Module object).

One question is what to do with threads? Does each thread have its own
"thread global" environment? Seems plausible.

Other issues to consider is that module imports have dynamic behaviour.
They can be evaluated in nested scopes and under conditional statements:

   if test:
      import Foo
   else:
      import Bar

So it is undesirable to require them to be statically known and linked.
That means we can't use Haskell's import facility to implement Python's
import facility (in its full glory). A promising workaround is to use
dynamic loading via something like the plugins library. The idea would
be that each compiled module exports a single entity called, say,
init, which would have a type like:

   init :: Eval Object

The resulting object would be a module Module that contains a
dictionary mapping all its members to objects.

So a Python statement like:

   import Foo

would be compiled to:

   obj <- importModule "Foo"
   _s_Foo <- var "Foo"
   _s_Foo =: obj

or maybe:

   _s_Foo <- importModule "Foo"

where

   importModule :: String -> Eval Object

or

   importModule :: String -> Eval ObjectRef

with pseudo code:

   importModule name = do
      maybeImported <- lookupModule name
      case maybeImported of
         Just obj -> return obj
         Nothing -> do
            tryCompiled <- compileModule name
            case tryCompiled of
               Left e -> raise ("compilation error: " ++ show e)
               Right obj -> return obj

   compileModule :: String -> Eval Object
   compileModule name = do
      maybePath <- findModulePath name
      case maybePath of
         Nothing -> raise ("could not find module")
         Just path -> liftIO $ load path "init"

assuming:


   load :: FilePath -> String -> IO a
