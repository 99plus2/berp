Random thoughts whilst implementing berp.

--------------------------------------------------------------------------------

The library StringTable.Atom looks like it might be useful for
implementing string lookups.

--------------------------------------------------------------------------------

Compiling variables.

Ideally we would compile Python variables into Haskell variables. But Python
variables can be updated inside loops and closures, whereas Haskell variables
cannot. The next best thing is to use IORefs. A promising scheme is to 
compile each Python variable into a Haskell variable bound to an IORef. A read
from a Python variable becomes a readIORef, likewise writeIORef.

A few problems still remain. Python's scoping rules are slightly different to
Haskell. 

Nested references to variables can be annotated as "global" and "nonlocal", otherwise
a nested assignment creates a new local variable. The main issue is that Python
variable binding sites are potentially ambiguous (there is no separate syntax
to introduce a new variable - assignment is used, but this is problematic when
another variable of the same name is already in scope).

Here is an example:

x = 12
def f():
   x = 144
   def g():
     global x
     x = 42
   g()
f()
print(x)

This prints 42. If you remove the "global x" then the program prints 12. 
Note that the "global x" causes the innermost x to refer to the outermost x, thus
skipping the one in the middle. Haskell's nested scoping does not have an
equivalent way to skip enclosing scope.

One possible way to avoid this is to rename global variables, in psuedo
compiled haskell:

global_x = newVariable "x" 
writeIORef global_x 12
f = def (\[] -> do
   x = newVariable "x"
   writeIORef x 144 
   g = def (\[] -> do
     writeIORef global_x 42
     )
   g @@ []
   )
f @@ []
print @@ [global_x]

This looks good but has problems with "nonlocal".

--------------------------------------------------------------------------------

Implementing generators/yield

data ControlStack =
   ...
   GeneratorContinuation
   { generator_return :: Object -> Eval ()
   , generator_object :: Object 
   , control_stack_tail :: ControlStack
   }
   ...

data Object =
   ...
   Generator
   { object_continuation :: IORef (Eval ())
   , object_stack_context :: IORef (ControlStack -> ControlStack)
   , object_identity :: Identity
   , object_type :: Object
   , object_dict :: Dictionary
   }
   ...

yield :: Object -> Eval ()
yield obj = do
   callCC $ \next -> do
      (generatorObj, context) <- unwindYieldContext
      liftIO $ writeIORef (object_continuation generatorObj) next 
      liftIO $ writeIORef (object_control_context generatorObj) context 
      generator_return top obj 

-- the next method for generators
next :: [Object] -> Eval Object
next (obj:_) = 
   callCC $ \afterYield -> 
      case obj of
         Generator {} -> 
            result <- do 
               push $ GeneratorContinuation afterYield obj
               stackContext <- liftIO $ readIORef $ object_stack_context obj
               modifyStack stackContext 
               liftIO $ readIORef $ object_continution obj 

-- used in the compilation of def generators
funGenerator :: Eval Object -> Eval Object
funGenertor body = do
   contRef <- liftIO $ newIORef (body >> return ())
   stackRef <- liftIO $ newIORef id
   generator contRef stackRef 

How much stack is saved when a yield occurs?

The Python PEP 0255 is not entirely clear about that:

  "If a yield statement is encountered, the state of the function is
   frozen, and the value of expression_list is returned to .next()'s
   caller.  By "frozen" we mean that all local state is retained,
   including the current bindings of local variables, the instruction
   pointer, and the internal evaluation stack:  enough information is
   saved so that the next time .next() is invoked, the function can
   proceed exactly as if the yield statement were just another external
   call."

What is the "internal evaluation stack"?

This example demonstrates that we don't save all the stack:

>>> def f():
...    yield 1
...    yield 2
...    1 / 0
...    yield 3
... 
>>> try:
...    i = f()
...    print(next(i))
... except:
...    print("here")
... 
1
>>> next(i)
2
>>> next(i)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 4, in f
ZeroDivisionError: int division or modulo by zero
>>> next(i)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration
>>> next(i)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration

As can be seen, the try/except is not preserved when the generator is called
subsequently.

Importantly we can also see that an exception raised during the execution
of a generator causes the generator to end and subsequent next() calls
result in StopIteration. 

--------------------------------------------------------------------------------

Comprehensions / generators

We have four kinds:
   - Generator expressions.
   - List comprehensions.
   - Set comprehensions.
   - Dict comprehensions.

Basic idea is to turn them all into generators, and for list, set and dict,
wrap the output of the generator in a constructor.

That is:

   [ x + 1 for x in e ]

becomes:

   list(x + 1 for x in e)

Then we compile generators into for loops:

So:

   list(x + 1 for x in e)

becomes:

   def gen():
       for x in e:
           yield x + 1
   list(gen())

But this is a little bit costly due to the creation of a new function
and the unnecessary use of iterators. We can specialise the construction
of the set/list/dictionary.

So instead:

   [ x + 1 for x in e ]

becomes:

   v = list()
   for x in e:
      v.append(x + 1)

and v is the value of the whole expression.

Generator comprehensions need to be treated differently, because:

   (x + 1 for x in e)

should not be:

   for x in e:
      yield x + 1

because you get a yield outside a function. Instead it should be:

   def v():
      for x in e:
         yield x + 1
   v()

But it is overkill to define a function for this. Instead we want
to compile to a version which uses mkGenerator:

   mkGenerator [[ for x in e: yield x + 1 ]]

Syntax:

   exp_comp := [comp] | {comp}_set | {comp}_dict | (comp)
   comp     := exp forpart
   forpart  := for pat in exp rest
   rest     := epsilon | ifpart | forpart
   ifpart   := if exp rest

[[ [comp] ]] =

   var = list()
   [[ comp ]]_(var,List)

[[ {comp}_set ]] =

   var = set()
   [[ comp ]]_(var,Set)

[[ {comp}_dict ]] =

   var = dict()
   [[ comp ]]_(var,Dict)

[[ (comp) ]] = mkGenerator ([[ comp ]]_(?,Gen))

[[ exp forpart ]]_(var,type) =

   [[ forpart ]]_(updater(type,var,result))

[[ for pat in exp rest ]]_(result) =

   for pat in exp:
       [[ rest ]]_(result)

[[ epsilon ]]_(result) = result

[[ if exp rest ]]_(result) =

   if exp:
       [[ rest ]]_(result)
