Random thoughts whilst implementing berp.

--------------------------------------------------------------------------------

The library StringTable.Atom looks like it might be useful for
implementing string lookups.

--------------------------------------------------------------------------------

Compiling variables.

Ideally we would compile Python variables into Haskell variables. But Python
variables can be updated inside loops and closures, whereas Haskell variables
cannot. The next best thing is to use IORefs. A promising scheme is to 
compile each Python variable into a Haskell variable bound to an IORef. A read
from a Python variable becomes a readIORef, likewise writeIORef.

A few problems still remain. Python's scoping rules are slightly different to
Haskell.

Nested references to variables can be annotated as "global" and "nonlocal", otherwise
a nested assignment creates a new local variable. The main issue is that Python
variable binding sites are potentially ambiguous (there is no separate syntax
to introduce a new variable - assignment is used, but this is problematic when
another variable of the same name is already in scope).

Here is an example:

x = 12
def f():
   x = 144
   def g():
     global x
     x = 42
   g()
f()
print(x)

This prints 42. If you remove the "global x" then the program prints 12. 
Note that the "global x" causes the innermost x to refer to the outermost x, thus
skipping the one in the middle. Haskell's nested scoping does not have an
equivalent way to skip enclosing scope.

One possible way to avoid this is to rename global variables, in psuedo
compiled haskell:

global_x = newVariable "x" 
writeIORef global_x 12
f = def (\[] -> do
   x = newVariable "x"
   writeIORef x 144 
   g = def (\[] -> do
     writeIORef global_x 42
     )
   g @@ []
   )
f @@ []
print @@ [global_x]

This looks good but has problems with "nonlocal".

--------------------------------------------------------------------------------

Implementing generators/yield

data ControlStack =
   ...
   GeneratorContinuation
   { generator_return :: Object -> Eval ()
   , generator_object :: Object 
   , control_stack_tail :: ControlStack
   }
   ...

data Object =
   ...
   Generator
   { object_continuation :: IORef (Eval ())
   , object_stack_context :: IORef (ControlStack -> ControlStack)
   , object_identity :: Identity
   , object_type :: Object
   , object_dict :: Dictionary
   }
   ...

yield :: Object -> Eval ()
yield obj = do
   callCC $ \next -> do
      (generatorObj, context) <- unwindYieldContext
      liftIO $ writeIORef (object_continuation generatorObj) next 
      liftIO $ writeIORef (object_control_context generatorObj) context 
      generator_return top obj 

-- the next method for generators
next :: [Object] -> Eval Object
next (obj:_) = 
   callCC $ \afterYield -> 
      case obj of
         Generator {} -> 
            result <- do 
               push $ GeneratorContinuation afterYield obj
               stackContext <- liftIO $ readIORef $ object_stack_context obj
               modifyStack stackContext 
               liftIO $ readIORef $ object_continution obj 

-- used in the compilation of def generators
funGenerator :: Eval Object -> Eval Object
funGenertor body = do
   contRef <- liftIO $ newIORef (body >> return ())
   stackRef <- liftIO $ newIORef id
   generator contRef stackRef 

How much stack is saved when a yield occurs?

The Python PEP 0255 is not entirely clear about that:

  "If a yield statement is encountered, the state of the function is
   frozen, and the value of expression_list is returned to .next()'s
   caller.  By "frozen" we mean that all local state is retained,
   including the current bindings of local variables, the instruction
   pointer, and the internal evaluation stack:  enough information is
   saved so that the next time .next() is invoked, the function can
   proceed exactly as if the yield statement were just another external
   call."

What is the "internal evaluation stack"?

This example demonstrates that we don't save all the stack:

>>> def f():
...    yield 1
...    yield 2
...    1 / 0
...    yield 3
... 
>>> try:
...    i = f()
...    print(next(i))
... except:
...    print("here")
... 
1
>>> next(i)
2
>>> next(i)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 4, in f
ZeroDivisionError: int division or modulo by zero
>>> next(i)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration
>>> next(i)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration

As can be seen, the try/except is not preserved when the generator is called
subsequently.

Importantly we can also see that an exception raised during the execution
of a generator causes the generator to end and subsequent next() calls
result in StopIteration. 

--------------------------------------------------------------------------------

Comprehensions / generators

We have four kinds:
   - Generator expressions.
   - List comprehensions.
   - Set comprehensions.
   - Dict comprehensions.

Basic idea is to turn them all into generators, and for list, set and dict,
wrap the output of the generator in a constructor.

That is:

   [ x + 1 for x in e ]

becomes:

   list(x + 1 for x in e)

Then we compile generators into for loops:

So:

   list(x + 1 for x in e)

becomes:

   def gen():
       for x in e:
           yield x + 1
   list(gen())

But this is a little bit costly due to the creation of a new function
and the unnecessary use of iterators. We can specialise the construction
of the set/list/dictionary.

So instead:

   [ x + 1 for x in e ]

becomes:

   v = list()
   for x in e:
      v.append(x + 1)

and v is the value of the whole expression.

Generator comprehensions need to be treated differently, because:

   (x + 1 for x in e)

should not be:

   for x in e:
      yield x + 1

because you get a yield outside a function. Instead it should be:

   def v():
      for x in e:
         yield x + 1
   v()

But it is overkill to define a function for this. Instead we want
to compile to a version which uses mkGenerator:

   mkGenerator [[ for x in e: yield x + 1 ]]

Syntax:

   exp_comp := [comp] | {comp}_set | {comp}_dict | (comp)
   comp     := exp forpart
   forpart  := for pat in exp rest
   rest     := epsilon | ifpart | forpart
   ifpart   := if exp rest

[[ [comp] ]] =

   var = list()
   [[ comp ]]_(var,List)

[[ {comp}_set ]] =

   var = set()
   [[ comp ]]_(var,Set)

[[ {comp}_dict ]] =

   var = dict()
   [[ comp ]]_(var,Dict)

[[ (comp) ]] = mkGenerator ([[ comp ]]_(?,Gen))

[[ exp forpart ]]_(var,type) =

   [[ forpart ]]_(updater(type,var,result))

[[ for pat in exp rest ]]_(result) =

   for pat in exp:
       [[ rest ]]_(result)

[[ epsilon ]]_(result) = result

[[ if exp rest ]]_(result) =

   if exp:
       [[ rest ]]_(result)

--------------------------------------------------------------------------------

Compiling modules.

Syntax:

   import mod
   import mod as alias
   from mod import item_list
   from mod import *

Issues:

The first time a module is imported into a running application it is "executed",
which may have side effects (eg top-level statements). Subsequent loads do not
execute those effects again. Thus the loaded module is cached. A corollary is
that modules are identified uniquely by their name.

Python supports "packages" which make the story slightly more complex. To
simplify things it might make sense to ignore packages in the first pass.

An import of a module binds one or more variables in the local scope of the
importing statement. This is
problematic for the "from mod import *" syntax, because it does not name the
variables that it binds - they are simply the variables exported by the
imported module. So far our scheme has been to compile
Python variable names into Haskell variable names (where the Haskell variable
is bound to an IORef). For performance reasons we are trying to avoid using
strings (and an explicit environment) to handle variable names - although such
an approach would make it relatively easy to handle this tricky kind of import.

Fortunately this tricky kind of import is only allowed at the top level of
a module. Given this limitation, a compromise might be possible. The idea
is to maintain a global environment, which maps (string) names to object
references (IORef Object, aka ObjectRef):

   type GlobalEnv = Map String ObjectRef

All variables referred to in a module must be declared first in the
compiled Haskell code. Top level
declarations can check the global environment. If a variable of the same name
was imported then the corresponding ObjectRef should be retrieved from the
table. Otherwise a new ObjectRef should be allocated and the table should
be updated. Reads from variables remain unchanged.

For instance, the Python code (at the top level)

   x = 12
   print(x)

is compiled to:

   = do _s_x <- var "x"
        _s_x =: 12
        _t_0 <- read _s_print
        _t_1 <- read _s_x
        _t_0 @@ [_t_1]

where the 'var' primitive does the variable declaration, and has type:

   var :: Ident -> Eval ObjectRef

which would be something like this in pseudo-code:

   var s = do
      maybeGlobal <- lookupGlobalEnv s
      case maybeGlobal of
         Nothing -> do
            ref <- newIORef (error $ "undefined variable: " ++ s)
            updateGlobalEnv s ref
            return ref
         Just ref -> return ref

An imported module should update the global environment with the variables
that are imported from it. In the case of the "import *", all variables are
imported, whereas the other kinds of imports are more restrictive. For instance,
"import Foo" just introduces the "Foo" variable into scope (which will be
bound to a Module object).

Side note: if we ever wanted to we could presumably extend this scheme to support
'from Foo import *' at other nesting levels, by including a suitably nested
variable enviroment.

One question is what to do with threads? Does each thread have its own
"thread global" environment? Seems plausible.

Other issues to consider is that module imports have dynamic behaviour.
They can be evaluated in nested scopes and under conditional statements:

   if test:
      import Foo
   else:
      import Bar

So it is undesirable to require them to be statically known and linked.
That means we can't use Haskell's import facility to implement Python's
import facility (in its full glory). (Though a static import facility
could be supported and may be a useful extension). A promising workaround is to use
dynamic loading via something like the plugins library. The idea would
be that each compiled module exports a single entity called, say,
init, which would have a type like:

   init :: Eval Object

The resulting object would be a Module that contains a
dictionary mapping all its members to objects.

So a Python statement like:

   import Foo

would be compiled to:

   obj <- importModule "Foo"
   _s_Foo <- var "Foo"
   _s_Foo =: obj

or maybe (this is probably better):

   _s_Foo <- importModuleRef "Foo"

where

   importModule :: String -> Eval Object

and/or

   importModuleRef :: String -> Eval ObjectRef

with pseudo code:

   -- this just handles the simple case of: import Foo
   importModule name = do
      maybeImported <- lookupModule name
      case maybeImported of
         Just obj -> return obj
         Nothing -> do
            tryCompiled <- compileModule name
            case tryCompiled of
               Left e -> raise ("compilation error: " ++ show e)
               Right obj -> return obj

   compileModule :: String -> Eval Object
   compileModule name = do
      maybePath <- findModulePath name
      case maybePath of
         Nothing -> raise ("could not find module")
         Just path -> do
            compiled <- isCompiled path
            if compiled
               then liftIO $ load path "init"
               else do
                  compileResult <- compileToObj path
                  case compileResult of
                     Nothing -> liftIO $ load path "init"
                     Just err -> raise err

assuming:

   load :: FilePath -> String -> IO a

or something like that.

Each Python module should be compiled to a Haskell binding defining
and init function which is the only variable exported from the
Haskell module:

   init :: Eval Object
   init = do
      ... compiled stuff ...
      -- should be hashed strings below
      mkModule [("x", _s_x), ..., ("z", s_z)]

where mkModuleObject builds the object for the module from
the top variables defined in it:

   mkModule :: [(Hashed String, ObjectRef)] -> Eval Object

We have to compile the Python code to object code and
then dynamically load the object code. This raises the question:

   Should we call the compiler (from the running program) as a shell call,
   or should we compile the compiler into the runtime library?

A shell call keeps the runtime and the compiler separate but at what advantage?
Will make the resulting executable smaller. But we could in theory
dynamically link the compiler to the excecutable. The space saving is not so
compelling because we still need to have the compiler around anyway. Nonetheless
static linking the compiler to the runtime would be undesirable. Does GHC support
dynamic linking everywhere?

It might be (slightly?) faster for the compiler to be called directly rather
than from a shell call. It might also be more portable.

Plan: see if we can compile the compiler into the runtime. See if it works and
see if the size of executables is okay. Hope for dynamic linking to work.

This will require us to build a berp (compiler) library from the cabal file.
Both the command line front end and the runtime will link to the library.

What about the main function?

Simple solution: the berp executable just dynamically loads the module that
was mentioned on the command line, something like:

   main :: IO ()
   main = do
      args <- getArgs
      let pySrc = getPySrc args
      init <- importModule pySrc
      runStmt init

How should the interpreter work? Maybe we can also use dynamic linking. The idea
is to compile each new statement into a temporary module and then dynamically
load it into the running program. The main issue to solve is how to bind the
free variables in the statement to their values from the running program? One
possible solution is to compile each statement into a function (closure) that
binds all the free variables. Something akin to the way we propose to handle
'from Foo import *'. It seems appealing to try to implement the interpreter this
way. Perhaps there is a performance issue due to loading times? Maybe we can
avoid touching the file system?

We could use the dynamic import facility to link the compiled program to the base library. That is
we compile a special module in the base library and dynamically load it a runtime. Or maybe it is just
better to statically import it? Again the availability of dynamic linking makes a difference to the
size of the resulting executable.

It looks like dynamic loading might invalidate any global state in the program.
Though it might be a bug, currently the stdandard IO devices do not seem to
persist as expected across a dynamic load (this is evident when the stdout is
redirected on the shell, after a dynamic load it seems the redirection is
lost and the output disappears). There are a few cases of global state in the
current implementation which could do with a revision in light of this
discovery. We use unsafePerformIO to make some otherwise effectful operations into
globals. This is safe because we are careful to ensure that the effects are
benign, such as allocating IORefs, but even so, the use of unsafe operations
seems less than ideal. A few observations:
   - If these things are truly constant, then we should never need the IO monad. Hopefully global
     constant bindings should suffice, immutable data structures.
   - For most/all such global constants, the point of making them global is to simplify
     scoping issues (they are in scope everywhere). An alternative approach is to bring them
     into scope by importing them, just as the import mechanism will bring other things into scope.
     We could pretend that every Python module has an implicit:

        from builtins import *

     at the top, where builtins is a special module which is part of the base library
     implementation. This might ultimately be a cleaner solution to the problem. Then everything
     will be in the Eval monad, and there will be no unsafePerformIO. However, a more static
     approach might be more efficient as it seems that dynamic linking is a little bit slow.
     Obviously berp will have to know how to find builtins.o in order to link it in.

Module naming strategy. Given a module called Foo.py, what is the name of the resulting Haskell module?
Haskell modules have an internal name and the filename is somewhat ancilliary. However, it tends to make
things easy for GHC if the module name is the same as the file name. This can cause trouble because of
capitalisation (and maybe there are other issues). The proposal is to prepend "Berp_" onto the front of every
name. For example, Foo.py will become (Berp_Foo.hs, module Berp_Foo), whereas foo.py will
become (Berp_foo.py, module Berp_foo). Even more name mangling might be needed if python allows characters in
its name that are not allowed in Haskell modules. I'm not aware of any other issues at the moment, but maybe
there are unusual things like unicode issues to consider.

Small problem found: we don't currently do a proper job of bound methods. This causes trouble with imports
like so:
    import Foo
    Foo.f()
We treat this like this was a method call of f on Foo, which means we add Foo as the first argument,
which is not correct. We need to distinguish between method lookups and module attribute lookups.

Implementation strategy:

   1. Add a module data constructor to the Object type. Done.
   2. Modify compilation of a Python module to use the init binding, (Done)
      and implement mkModule (Done). Move compiler code into library (Done).
      Implement a basic importModule (Done).
      Need to handle the main function to evaluate the whole program, this will
      basically just call importModule (Done).
   3. Get module name mangling to work. (Done)
   4. Get the simple case of "import Foo" to work. Don't worry about
      caching imports. Don't worry about the search path. Just find
      modules in the current directory. (Done)
   5. Get it working in the interpreter. (Maybe skip if tricky).
   6. Add import caching (Pending).
   7. Implement the more difficult case of "from Foo import (x,y,z)"
   8. Implement the hardest case of "from Foo import *".
   9. Add search path for files.
   --- stop here and release the code ---
   10. Consider what's needed for packages.
